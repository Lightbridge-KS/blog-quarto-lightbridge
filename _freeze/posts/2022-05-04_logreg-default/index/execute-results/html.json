{
  "hash": "835922e8e4a6c8bec625030df548587d",
  "result": {
    "markdown": "---\ntitle: \"Predicting Credit Card Default Status\"\nsubtitle: \"A Logistic Regression Model using R.\"\ndescription: \"Playing with logistic regression model with `Default` dataset\"\ndate: \"2022-05-04\"\ndraft: false # Will render locally only\ncategories: [\"R\", \"Stats\", \"ML\"]\ntags: \n  - \"logreg\"\nimage: feature.jpg\nbibliography: \"packages.bib\"\nexecute: \n  cache: true\n---\n\n\n\n\nMy motivation and code I used in here is an adaptation from Dr.Julia Silage's blog [bird-baths](https://juliasilge.com/blog/bird-baths/). I've applied the similar modeling process to `Default` dataset from `{ISLR}` package [@R-ISLR].\n\nThe goal is to build logistic regression model to predict `default` status.\n\n# Explore Data\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-1_2920d166874ffbdc971908b81491d440'}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(ISLR)\ntheme_set(theme_bw())\n```\n:::\n\n\nLet's take a look at the `Default` data set. It has 2 numeric variables: `balance` and `income`; and 2 factor variables: `default` and `student`\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-2_d0a6811980ed13f309d9ffb8e41f1457'}\n\n```{.r .cell-code}\nsummary(Default)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n default    student       balance           income     \n No :9667   No :7056   Min.   :   0.0   Min.   :  772  \n Yes: 333   Yes:2944   1st Qu.: 481.7   1st Qu.:21340  \n                       Median : 823.6   Median :34553  \n                       Mean   : 835.4   Mean   :33517  \n                       3rd Qu.:1166.3   3rd Qu.:43808  \n                       Max.   :2654.3   Max.   :73554  \n```\n:::\n:::\n\n\nDetailed summary can be done with `skimr::skim()`\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-3_9aabb4b7fe39698833f72d0fe6d8aa53'}\n\n```{.r .cell-code}\nskimr::skim(Default)\n```\n\n::: {.cell-output-display}\nTable: Data summary\n\n|                         |        |\n|:------------------------|:-------|\n|Name                     |Default |\n|Number of rows           |10000   |\n|Number of columns        |4       |\n|_______________________  |        |\n|Column type frequency:   |        |\n|factor                   |2       |\n|numeric                  |2       |\n|________________________ |        |\n|Group variables          |None    |\n\n\n**Variable type: factor**\n\n|skim_variable | n_missing| complete_rate|ordered | n_unique|top_counts          |\n|:-------------|---------:|-------------:|:-------|--------:|:-------------------|\n|default       |         0|             1|FALSE   |        2|No: 9667, Yes: 333  |\n|student       |         0|             1|FALSE   |        2|No: 7056, Yes: 2944 |\n\n\n**Variable type: numeric**\n\n|skim_variable | n_missing| complete_rate|     mean|       sd|     p0|      p25|      p50|      p75|     p100|hist  |\n|:-------------|---------:|-------------:|--------:|--------:|------:|--------:|--------:|--------:|--------:|:-----|\n|balance       |         0|             1|   835.37|   483.71|   0.00|   481.73|   823.64|  1166.31|  2654.32|▆▇▅▁▁ |\n|income        |         0|             1| 33516.98| 13336.64| 771.97| 21340.46| 34552.64| 43807.73| 73554.23|▂▇▇▅▁ |\n:::\n:::\n\n\nPlease note that there are no missing values, which is great!\n\n::: callout-tip\n### Goal\n\n**The goal** here is to build a logistic regression model to predict `default`.\n:::\n\n## Plot Relationship\n\nI will explore the relationship between `default` and other variables (`income`, `balace`, `student`). Let's make some plots!\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-4_1768d8bd6b3c4d6ade1c3408f2232745'}\n\n```{.r .cell-code}\nDefault %>% \n  ggplot(aes(balance, income, color = default)) + \n  geom_point(alpha = 0.4) +\n  scale_color_brewer(palette = \"Set1\", direction = -1) +\n  labs(title = \"Default status by income and balance\") \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nBy visual inspection, `balance` looks like a better predictor for `default` than `income`.\n\n\n::: {.cell hash='index_cache/html/p1_1c04fb49e0f62df9fff170ca1ee2b9ac'}\n\n```{.r .cell-code}\np1 <- Default %>% \n  ggplot(aes(balance, default, fill = student)) +\n  geom_boxplot(alpha = 0.8) +\n  scale_fill_brewer(palette = \"Dark2\") +\n  labs(title = \"Distribution of default\",\n       subtitle = \"by balance and student status\",\n       caption = \"Data from ISLR package\") \n  \np1 \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/p1-1.png){width=672}\n:::\n:::\n\n\nThis plot shows that `balance` and `student` seem to be a decent predictor of `default`.\n\n# Simple Model\n\n::: callout-note\n\n**Goal:** Outcome variable = `default` (factor)\n\n:::\n\nFirst, I will use `glm()` function to build logistic regression model using all predictors.\n\n\n::: {.cell hash='index_cache/html/glm_fit_simple_32f04a947b90e47e5f0397e28e10b606'}\n\n```{.r .cell-code}\nglm_fit_all <- glm(default ~ ., data = Default, family = binomial)\nsummary(glm_fit_all)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = default ~ ., family = binomial, data = Default)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.4691  -0.1418  -0.0557  -0.0203   3.7383  \n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -1.087e+01  4.923e-01 -22.080  < 2e-16 ***\nstudentYes  -6.468e-01  2.363e-01  -2.738  0.00619 ** \nbalance      5.737e-03  2.319e-04  24.738  < 2e-16 ***\nincome       3.033e-06  8.203e-06   0.370  0.71152    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2920.6  on 9999  degrees of freedom\nResidual deviance: 1571.5  on 9996  degrees of freedom\nAIC: 1579.5\n\nNumber of Fisher Scoring iterations: 8\n```\n:::\n:::\n\n\n\n\nCoefficient of `income` is not significant (p = 0.7115203). Let's try remove `income` out.\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-5_a9c1093bbb661e3259520c9ee6825b18'}\n\n```{.r .cell-code}\nglm_fit_st_bal <- glm(default ~ student + balance, \n    data = Default, \n    family = binomial)\nglm_fit_st_bal\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:  glm(formula = default ~ student + balance, family = binomial, \n    data = Default)\n\nCoefficients:\n(Intercept)   studentYes      balance  \n -10.749496    -0.714878     0.005738  \n\nDegrees of Freedom: 9999 Total (i.e. Null);  9997 Residual\nNull Deviance:\t    2921 \nResidual Deviance: 1572 \tAIC: 1578\n```\n:::\n:::\n\n\nModel that include only `student` and `balance` has **lower** estimation of test error (i.e., AIC & BIC).\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-6_e9a74bc24d1bfa311be7c58776fa0f05'}\n\n```{.r .cell-code}\nlist(\"All Predictors\" = glm_fit_all,\n     \"student + balance\" = glm_fit_st_bal) %>% \n  map_dfr( \n    broom::glance, \n    .id = \"Model\") %>% \n  select(Model, AIC, BIC)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 3\n  Model               AIC   BIC\n  <chr>             <dbl> <dbl>\n1 All Predictors    1580. 1608.\n2 student + balance 1578. 1599.\n```\n:::\n:::\n\n\n# Modeling Process\n\nNow, let's apply full modeling process with `tidymodels`.\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-7_1a1960a822ce0da608e49a7eabf95b28'}\n\n```{.r .cell-code}\nlibrary(tidymodels)\n```\n:::\n\n\n## Split Data\n\nFirst, we need to spending data budgets in 2 portions: *training* and *testing* data.\n\n\n::: {.cell hash='index_cache/html/Default_split_57f1b030f53410a88cb62e56949062af'}\n\n```{.r .cell-code}\nset.seed(123)\n# Split to Test and Train\nDefault_split <- initial_split(Default, strata = default)\n\nDefault_train <- training(Default_split) # 75% to traning data\nDefault_test <- testing(Default_split) # 25% to test data\n\nDefault_split\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<Analysis/Assess/Total>\n<7500/2500/10000>\n```\n:::\n:::\n\n\n## Resampling Method\n\nI will use **10-Fold Cross-Validation** to resample the training data.\n\n\n::: {.cell hash='index_cache/html/Default_folds_85fdcca076019a181c198de09966b0d4'}\n\n```{.r .cell-code}\nset.seed(234)\n\nDefault_folds <- vfold_cv(Default_train, v = 10, strata = default)\nDefault_folds \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#  10-fold cross-validation using stratification \n# A tibble: 10 × 2\n   splits             id    \n   <list>             <chr> \n 1 <split [6750/750]> Fold01\n 2 <split [6750/750]> Fold02\n 3 <split [6750/750]> Fold03\n 4 <split [6750/750]> Fold04\n 5 <split [6750/750]> Fold05\n 6 <split [6750/750]> Fold06\n 7 <split [6750/750]> Fold07\n 8 <split [6750/750]> Fold08\n 9 <split [6750/750]> Fold09\n10 <split [6750/750]> Fold10\n```\n:::\n:::\n\n\nNote that I use `strata = default` because the frequency of each class is quite difference.\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-8_4abf80d9088f06d60f73fe908310b7ac'}\n\n```{.r .cell-code}\nDefault %>% count(default)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  default    n\n1      No 9667\n2     Yes  333\n```\n:::\n:::\n\n\nFor each folds, 6750 rows will spend on fitting models and 750 spend on analysis of model performance:\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-9_4c8a036b1e3eea1f0516d9f718036720'}\n\n```{.r .cell-code}\nDefault_folds$splits[[1]] \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<Analysis/Assess/Total>\n<6750/750/7500>\n```\n:::\n:::\n\n\n## Specification\n\n### Model Specification\n\n\n::: {.cell hash='index_cache/html/glm_spec_0a855838e021310b8dfc522da7ec81b6'}\n\n```{.r .cell-code}\nglm_spec <- logistic_reg()\nglm_spec\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n```\n:::\n:::\n\n\n### Feature Engineering Specification\n\n\n::: {.cell hash='index_cache/html/rec_basic_8d2afa528615d3e6f9a415bb3c84f9bf'}\n\n```{.r .cell-code}\nrec_basic <- recipe(default ~ ., data = Default) %>% \n  step_dummy(all_nominal_predictors())\n\nsummary(rec_basic)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 4\n  variable type    role      source  \n  <chr>    <chr>   <chr>     <chr>   \n1 student  nominal predictor original\n2 balance  numeric predictor original\n3 income   numeric predictor original\n4 default  nominal outcome   original\n```\n:::\n:::\n\n\n**Preview of engineered training data**\n\nYou can see that `student` was replaced by `student_Yes` with 0-1 encoding. What does it mean?\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-10_1c495a8072c5f92d1bf64be160529625'}\n\n```{.r .cell-code}\nrec_basic %>% \n  prep(log_change = T) %>% \n  bake(new_data = Default_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nstep_dummy (dummy_72FQt): \n new (1): student_Yes\n removed (1): student\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 7,500 × 4\n   balance income default student_Yes\n     <dbl>  <dbl> <fct>         <dbl>\n 1    730. 44362. No                0\n 2    817. 12106. No                1\n 3   1074. 31767. No                0\n 4    529. 35704. No                0\n 5    786. 38463. No                0\n 6    826. 24905. No                0\n 7    809. 17600. No                1\n 8   1161. 37469. No                0\n 9      0  29275. No                0\n10      0  21871. No                1\n# … with 7,490 more rows\n```\n:::\n:::\n\n\nFrom `contrast()` function we can see the dummy encoding of the `student` (factor) variable: No = 0, Yes = 1.\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-11_5f8fe61971165e526be1ffed386be4d6'}\n\n```{.r .cell-code}\ncontrasts(Default$student)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    Yes\nNo    0\nYes   1\n```\n:::\n:::\n\n\n### Workflow\n\n`workflow()` will bundle blueprint of feature engineering and model specification together.\n\n\n::: {.cell hash='index_cache/html/wf_basic_b80c8c50384dedb694a426f4e0adfaa7'}\n\n```{.r .cell-code}\nwf_basic <- workflow(rec_basic, glm_spec)\nwf_basic\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n```\n:::\n:::\n\n\n## Fit Model to Resampled Data\n\n\n::: {.cell hash='index_cache/html/rs_basic_12b94af96068a1561931e97ca0e6c011'}\n\n```{.r .cell-code}\ndoParallel::registerDoParallel()\n\nctrl_preds <- control_resamples(save_pred = TRUE)\n## Fit\nrs_basic <- fit_resamples(wf_basic, resamples =  Default_folds, control = ctrl_preds)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'rlang' was built under R version 4.1.2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'vctrs' was built under R version 4.1.2\n```\n:::\n\n```{.r .cell-code}\nhead(rs_basic)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 5\n  splits             id     .metrics         .notes           .predictions      \n  <list>             <chr>  <list>           <list>           <list>            \n1 <split [6750/750]> Fold01 <tibble [2 × 4]> <tibble [0 × 1]> <tibble [750 × 6]>\n2 <split [6750/750]> Fold02 <tibble [2 × 4]> <tibble [0 × 1]> <tibble [750 × 6]>\n3 <split [6750/750]> Fold03 <tibble [2 × 4]> <tibble [0 × 1]> <tibble [750 × 6]>\n4 <split [6750/750]> Fold04 <tibble [2 × 4]> <tibble [0 × 1]> <tibble [750 × 6]>\n5 <split [6750/750]> Fold05 <tibble [2 × 4]> <tibble [0 × 1]> <tibble [750 × 6]>\n6 <split [6750/750]> Fold06 <tibble [2 × 4]> <tibble [0 × 1]> <tibble [750 × 6]>\n```\n:::\n:::\n\n\n## ROC Curve\n\nFrom ROC curve we can see that it has pretty good upper-left bulging curve.\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-12_726df52450632de5cdca4914534b3bbe'}\n\n```{.r .cell-code}\naugment(rs_basic) %>% \n  roc_curve(truth = default, .pred_No) %>% \n  autoplot()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\nThis would result in AUC (area under the ROC curve) and accuracy close to 1.\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-13_bd4d80cf0518d8b3f9ac369ddd0f228f'}\n\n```{.r .cell-code}\ncollect_metrics(rs_basic)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \n1 accuracy binary     0.974    10 0.00194 Preprocessor1_Model1\n2 roc_auc  binary     0.953    10 0.00461 Preprocessor1_Model1\n```\n:::\n:::\n\n\n# Improve the Model\n\nAs we can see from the beginning, removing `income` from predictor result in better estimation of test error by AIC and BIC.\n\nNow, I will remove `income` from the recipes:\n\n\n::: {.cell hash='index_cache/html/rec_simple_0517858af5e986d86b184723d2c6356f'}\n\n```{.r .cell-code}\nrec_simple <- rec_basic %>% \n  remove_role(income, old_role = \"predictor\")\n\nsummary(rec_simple)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 4\n  variable type    role      source  \n  <chr>    <chr>   <chr>     <chr>   \n1 student  nominal predictor original\n2 balance  numeric predictor original\n3 income   numeric <NA>      original\n4 default  nominal outcome   original\n```\n:::\n:::\n\n\nAnd I will update the WorkFlow\n\n\n::: {.cell hash='index_cache/html/wf_simple_e940189b71f2609bb9e488c024863b2e'}\n\n```{.r .cell-code}\nwf_simple <- workflow(rec_simple, glm_spec)\n```\n:::\n\n\nThen the rest is the same. So, I wrote a simple wrapper function to do it.\n\n\n::: {.cell hash='index_cache/html/update_workflow_2407a4d67a77dc29ce198b0a294ec7b1'}\n\n```{.r .cell-code}\nupdate_workflow <- function(wf) {\n\n  ctrl_preds <- control_resamples(save_pred = TRUE)\n  rs <- fit_resamples(wf, resamples = Default_folds, control = ctrl_preds)\n  rs\n  \n}\n\nrs_simple <- update_workflow(wf_simple)\n```\n:::\n\n::: {.cell hash='index_cache/html/rs_ls_9d677982728394d6ea082c7d86f6a232'}\n\n```{.r .cell-code}\nrs_ls <- list(\"All Predictors\" = rs_basic,\n              \"student + balance\" = rs_simple)\n```\n:::\n\n::: {.cell hash='index_cache/html/roc_df_9342ab58a6127153a2923cef363cfec3'}\n\n```{.r .cell-code}\nroc_df <- rs_ls %>% \n  map_dfr(~augment(.x) %>% roc_curve(truth = default, .pred_No),\n      .id = \"Predictors\"\n      )\n```\n:::\n\n\n## ROC curve of the improved model\n\nThis plot show **comparison of ROC curves** of the 2 logistic regression models.\n\n-   Red line shows model that use all predictors: `student`, `balance` and `income`.\n-   Blue line shows model that use 2 predictor: `student` and `balance`.\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-14_71ec012730ae53663f4dd4e19d662149'}\n\n```{.r .cell-code}\nroc_df %>% \n  ggplot(aes(1-specificity, sensitivity, color = Predictors)) +\n  geom_line(size = 1, alpha = 0.6, linetype = \"dashed\") +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dotted\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n::: {.cell hash='index_cache/html/unnamed-chunk-15_672d3ec57c3e79e0e0c37c2bcbd8f269'}\n\n```{.r .cell-code}\nrs_ls %>% \n  map_dfr(\n    collect_metrics, .id = \"Features\"\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 7\n  Features          .metric  .estimator  mean     n std_err .config             \n  <chr>             <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \n1 All Predictors    accuracy binary     0.974    10 0.00194 Preprocessor1_Model1\n2 All Predictors    roc_auc  binary     0.953    10 0.00461 Preprocessor1_Model1\n3 student + balance accuracy binary     0.974    10 0.00194 Preprocessor1_Model1\n4 student + balance roc_auc  binary     0.953    10 0.00461 Preprocessor1_Model1\n```\n:::\n:::\n\n\nYou can see that a simpler model result in a similar (or may be slightly improved) AUC. So it's reasonable to prefer it over more complicated model.\n\n# Evaluate Model\n\nIn this section, I will evaluate the simpler model with 2 predictors (`student`, `balance`).\n\n## Fit to Training data\n\nFirst, fit the model to training data.\n\n\n::: {.cell hash='index_cache/html/Default_fit_c373c1280519874a54f85f210d0f8c8e'}\n\n```{.r .cell-code}\nDefault_fit <- fit(wf_simple, Default_train)\nDefault_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:  stats::glm(formula = ..y ~ ., family = stats::binomial, data = data)\n\nCoefficients:\n(Intercept)      balance  student_Yes  \n  -10.85340      0.00578     -0.71313  \n\nDegrees of Freedom: 7499 Total (i.e. Null);  7497 Residual\nNull Deviance:\t    2158 \nResidual Deviance: 1139 \tAIC: 1145\n```\n:::\n:::\n\n\n## Use Testing data to Predict\n\nThen, use test data to predict `default`.\n\n\n::: {.cell hash='index_cache/html/Default_pred_566a1c6cc4fffcfa1da790acde7b20cd'}\n\n```{.r .cell-code}\nDefault_pred <- \n  augment(Default_fit, Default_test) %>% \n  bind_cols(\n    predict(Default_fit, Default_test, type = \"conf_int\")\n  )\n```\n:::\n\n::: {.cell hash='index_cache/html/p2_6e00f275dbe13f46a7108414d56b7700'}\n\n```{.r .cell-code}\nlibrary(latex2exp)\np2 <- Default_pred %>% \n  ggplot(aes(balance, .pred_Yes, fill = student)) +\n  geom_line(aes(color = student)) +\n  geom_ribbon(aes(ymin = .pred_lower_Yes, ymax = .pred_upper_Yes),\n              alpha = 0.3) +\n  scale_fill_brewer(palette = \"Dark2\") +\n  scale_color_brewer(palette = \"Dark2\") +\n  labs(y = TeX(\"$\\\\hat{p}(default)$\"),\n       caption = \"Area indicate 95% CI of the estimated probability\") +\n  labs(title = \"Predicted probability of default\", \n       subtitle = \"by logistic regression with 2 predictors\")\n\np2 \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/p2-1.png){width=672}\n:::\n:::\n\n\n\n\n\nThis plot show estimated probability of `default` if we know the values of predictors: `student` and `balance` by using logistic regression model fitted on training data. The prediction was made by plugging test data to the model.\n\n# Summary\n\n\n\n::: {.cell .column-screen-inset-shaded layout-nrow=\"1\" hash='index_cache/html/p1-p2_59bfd1312ad8ffc09661e9408106de1f'}\n::: {.cell-output-display}\n![](index_files/figure-html/p1-p2-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/p1-p2-2.png){width=672}\n:::\n:::\n\n\n::: callout-tip\n### Summary Plots\n\nThe left-sided plot showed default status by balance and student status as observed in the `Default` data set. After multivariate logistic regression model (`default` on `balance` and `student`) was fitted to the training data, the predicted probability of default using the model was shown in the right-sided plot.\n\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": null
  }
}